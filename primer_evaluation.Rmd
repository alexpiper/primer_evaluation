---
title: "Primer evaluation"
title: "Statistics"
author: "Alexander Piper"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Introduction


## Load packages
```{r setup}
#Set required packages
.cran_packages <- c("usethis",
                    "tidyverse",
                    "spider", 
                    "insect",
                    "ape",
                    "RColorBrewer",
                    "seqinr",
                    "patchwork",
                    "ShortRead",
                    "foreach",
                    "doParallel",
                    "TmCalculator",
                    "castor",
                    "furrr",
                    "tictoc", 
                    "UpSetR")

.bioc_packages <- c("DECIPHER",
                    "ggtree",
                    "Biostrings")

.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
  BiocManager::install(.bioc_packages[!.inst], ask = F)
}

#Load all packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

# Github packages
devtools::install_github("alexpiper/taxreturn")
devtools::install_github("krassowski/complex-upset")
library(complex-upset)
library(taxreturn)
library(PrimerMiner)

# SOurce internal functions
source("R/helper_functions.R")
source("R/themes.R")
```

# Assemble pest list

* EPPO global database https://gd.eppo.int/ - DONE
* US APHIS - https://www.aphis.usda.gov/aphis/home/ - DONE
* QBank - https://qbank.eppo.int/arthropods/organisms - DONE
* Global invasive species database - http://www.iucngisd.org/gisd/search.php - DONE
* Global register of introduced or invasive species http://www.griis.org/ - DONE
* VectorBase: https://www.vectorbase.org/organisms - DONE
* DAWR top 40 - http://www.agriculture.gov.au/pests-diseases-weeds/plant - DONE
* PHA National biosecurity status report -  http://www.planthealthaustralia.com.au/national-programs/national-plant-biosecurity-status-report/ - DONE
* Ashfaq & Herbert 2016 - DNA barcodes for bio-surveillance: regulated and economically important arthropod plant pests - DONE
* CABI - https://t.co/LGjlFoOazd - DONE
* http://www.europe-aliens.org - DONE

```{r Curate pest lists}
dat <- list.files("primer_evaluation/pestlist/", pattern = ".csv", full.names = TRUE) %>%
  purrr::set_names() %>%
  map_dfr(read_csv, .id = "Source", col_types = cols("Species" = col_character())) %>%
  mutate(Source = str_remove(basename(Source), pattern="\\.csv")) %>%
  mutate(Species = Species %>%  
           str_remove_all("Ã¿") %>% #Resolve weird characters
           iconv(from = 'UTF-8', to = 'ASCII//TRANSLIT')%>% 
           str_remove_all("\\?") %>%
           str_remove_all("\\((.*?)\\)") %>% # remove things between brackets ie: Hygromia (Hygromia) cinctella
           str_squish() # remove excess whitespace
         ) %>%
  filter(str_count(Species, " ") > 0 ) %>% #Remove non-binomial 
  separate(Species, into=c("Genus", "Species"), sep=" ", extra="merge") %>% # Fix duplicated genus names
  mutate(Species = str_remove(Species, pattern=Genus) %>% str_squish()) %>%
  unite(col=Species, Genus, Species, sep = " ") %>%
  unique()

# Map to OTT taxonomy ids
db <- taxreturn::get_ott_taxonomy(dir="ott3.2", filter_unplaced = FALSE)

dat_resolved <- dat %>% mutate(mapped =taxreturn::map_to_ott(Species, db, resolve_synonyms=TRUE,  dir="ott3.2", filter_unplaced=TRUE, remove_na = FALSE, quiet=FALSE) ) %>%
  mutate(Species = mapped %>% str_remove("^.*;"),
         taxid = mapped %>% str_remove(";.*$") %>% str_remove("^.*\\|") )%>%
  mutate(taxid = na_if(taxid, "NA")) %>%
  dplyr::filter(!is.na(taxid)) %>%
  rownames_to_column("rows") %>%
  mutate(mapped = mapped %>% str_remove("^NA") %>% paste0(rows, .)) %>%  #Add dummy accession number
  dplyr::select(-rows)

lineage <- get_ott_lineage(dat_resolved$mapped, db) %>% 
  bind_cols(dat_resolved) %>% 
  dplyr::select(-mapped, -tax_name, -Species) %>% 
  rename_all(funs(str_to_sentence(.))) %>%
  filter(Class %in% c("Insecta", "Arachnida")) %>%
  drop_na()

#Write out final list of pests 
write_csv(lineage, "primer_evaluation/pestlist.csv")
```

## Figure 1

```{r Figure 1}
lineage <- read_csv("primer_evaluation/pestlist.csv") %>%
  distinct() %>%
  mutate(Source = Source %>% str_remove("export_")) %>%
  mutate(Source = case_when(
    Source == "griis" ~ "GRIIS",
    Source=="gisd" ~ "GISD",
    Source=="europealiens" ~ "DAISIE",
    Source=="eppo" ~ "EPPO",
    Source=="ashfaq" ~ "Ashfaq et al 2016",
    Source=="qbank" ~ "Q-Bank",
    Source=="cabi" ~ "CABI",
    Source=="pha" ~ "PHA",
    Source=="vectorbase" ~ "VectorBase",
    Source=="dawr40" ~ "DAWR Top 40"
  ))


library(ggupset)
gg.upset <- lineage %>%
  dplyr::select(Species, Source) %>%
  group_by(Species) %>%
  summarise(Source = list(Source)) %>%
  ggplot(aes(x=Source)) +
    geom_bar() +
    scale_x_upset(n_intersections = 25) +
    base_theme +
    theme(panel.grid = element_blank()) +
  labs(x = "Intersection between datasets", y="Intersection size")

gg.upset

# Total set size
gg.total <- lineage %>%
  mutate(Source = factor(Source, levels=rev(c("GRIIS", "DAISIE", "EPPO", "Ashfaq et al 2016", "Q-Bank", "CABI", "PHA", "VectorBase", "DAWR Top 40", "GISD")))) %>%
  mutate(label = case_when(
   Class == "Insecta" & Order %in% c("Diptera" ,"Coleoptera", "Hemiptera", "Hymenoptera", "Lepidoptera", "Orthoptera") ~ Order,
  Class == "Arachnida" ~ "Arachnida",
  Class == "Insecta" & !Order %in% c("Diptera" ,"Coleoptera", "Hemiptera", "Hymenoptera", "Lepidoptera", "Orthoptera") ~ "Other Insects"
  )) %>%
  ggplot(aes(x=Source, fill=label))+
  geom_bar() +
  scale_fill_brewer(palette="Paired") + 
  base_theme +
  labs(x=NULL, y="Total records", fill="Taxon") +
  coord_flip()+
  theme(legend.position = "bottom") +
  scale_y_reverse()
  
# PCA of reference database
#Use unifrac and phylogenetic tree?


pca_dat <- lineage %>%
  dplyr::select(Species, Source) %>%
  distinct() %>%
  mutate(value = 1) %>%
  pivot_wider(names_from = Source, values_from=value, values_fill=0) %>%
  column_to_rownames("Species")


database_pca <- prcomp(vegan::vegdist(t(pca_dat), method="jaccard" ), scale. = TRUE)

gg.database_pca <-database_pca %>%
  broom::augment() %>%
    ggplot(aes(.fittedPC1, .fittedPC2, colour=.rownames, fill=.rownames)) +
    geom_point(size = 4, alpha = 0.8, shape=21, colour="black") +
  ggrepel::geom_text_repel(aes(label = .rownames, colour=.rownames))+
    labs(x = paste0("Principal component 1 (",  percent(percent_variation[1]), ")"), 
         y = paste0("Principal component 2 (",  percent(percent_variation[2]),")")) +
  geom_vline(xintercept = 0)+
  geom_hline(yintercept = 0) +
  base_theme +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") 

# Assemble figure 1

Fig1a <- gg.database_pca / gg.total + plot_layout(heights = c(2,1))

Fig1b <- gg.upset / grid::textGrob('')+ plot_layout(heights = c(10,1))

Fig1 <- Fig1a - Fig1b +  plot_annotation(tag_levels = 'A') 

Fig1

#Save figure 1
pdf(file="fig/Fig1_pestlist_summary.pdf", width = 11, height = 8 , paper="a4r")
  plot(Fig1)
try(dev.off(), silent=TRUE)
  
# Summaries for article text

# Unique taxa
lineage %>% 
  #select(-Source, Acc) %>%
  summarise(Species = n_distinct(Species),
            Genus = n_distinct(Genus),
            Family = n_distinct(Family),
            Order = n_distinct(Order),
            )
# Sum of reference DB's
lineage %>% 
  group_by(Source) %>%
  summarise(Species = n_distinct(Species)) %>%
  arrange(Species)

# Proportion of sequences unique
lineage %>%
  add_count(Source, name = "DB_total") %>%
  group_by(Species) %>%
  add_tally(name = "n_occurances") %>%
  ungroup() %>%
  filter(n_occurances==1) %>%
  group_by(Source, DB_total)%>%
  summarise(n = n_distinct(Species)) %>%
  mutate(freq = n / DB_total)%>%
  arrange(freq)
```

# Primer statistics

```{r primer binding and constraints}
#alignment was then manually curated in geneious primer
model <- readRDS("reference/folmer_fullength_model.rds")

# Be worth validating how the score is calculated?
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  select(name, strand, seq, citation, issues) %>%
  left_join(.$seq %>% purrr::map(get_binding_position, model, tryrc = TRUE, minscore=8) %>%
              bind_rows() %>%
              dplyr::rename(seq = primer), by="seq")%>%
  distinct()

primers <- primers %>%
  left_join(.$seq %>%
  purrr::map_df(get_primer_statistics, metrics="all", disambiguate=TRUE))

write_csv(primers, "primer_evaluation/primer_candidates.csv")

#Number of unique forward and reverse
primers %>% 
  dplyr::select(seq, strand) %>%
  distinct() %>%
  pull(strand) %>%
  table()

```

## Primer Presence in seqs

```{r check pres}
primerHits <- function(primer, fn, max.mismatch=0, with.indels=FALSE) {
      if(stringr::str_detect(primer, "I")) {
        message(paste0("Warning: Inosine (I) bases detected in primer ", primer," these will be converted to N!"))
        primer <- primer %>% str_replace_all("I", "N")
        }
    # Counts number of sequences in which the primer is found
    nhits <- vcountPattern(primer, sread(readFasta(fn)), max.mismatch=max.mismatch, fixed = FALSE, with.indels = with.indels)
    return(sum(nhits > 0))
}

primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  distinct()

target="reference/merged_final.fa.gz"

out <- vector("list", length=nrow(primers))
for (i in 1:nrow(primers)){
  
  if(primers$strand[i] == "F"){
    query <- primers$seq[i]
    
  } else  if(primers$strand[i] == "R"){
    query <- rc(primers$seq[i])
  }
  print(i)
  df <- tibble(
    name = primers$name[i],
    primer = query,
    strand = primers$strand[i],
    #Hamming distance (no indels in COI)
    h0 = primerHits(query, target, max.mismatch=0, with.indels=FALSE),
    h1 = primerHits(query, target, max.mismatch=1, with.indels=FALSE),
    h2 = primerHits(query, target, max.mismatch=2, with.indels=FALSE),
  )
  out[[i]] <- df
}

names(out) <- primers$seq
out <- bind_rows(out)
write_csv(out, "primer_evaluation/primer_presence.csv")

# Plotting
out <- read_csv("primer_evaluation/primer_presence.csv") %>% 
  pivot_longer(cols=starts_with(c("h", "l")),
               names_to = "measure",
               values_to = "seqs"
               ) %>%
  filter(measure=="h2") %>%
  mutate(name = factor(name)) %>%
  arrange(name)

gg.primerpresF <- out %>%
  left_join(primers) %>%
  filter(strand=="F") %>%
    mutate(name = fct_reorder(name, seqs, .desc = TRUE)) %>%
  ggplot(aes(x=name, y=seqs, fill=seqs)) + 
  geom_bar(stat="identity", position="dodge") +
  coord_flip() + 
  labs(x="Forward Primers", y=NULL)  +
  base_theme +
  scale_fill_gradient(low = "firebrick", high = "darkslateblue", 
                      na.value = "grey", oob = scales::squish) 

gg.primerpresR <- out  %>%
  left_join(primers) %>%
  filter(strand=="R") %>%
    mutate(name = fct_reorder(name, seqs, .desc = TRUE)) %>%
  ggplot(aes(x=name, y=seqs, fill=seqs)) + 
  geom_bar(stat="identity", position="dodge") +
  coord_flip() + 
  labs(x="Reverse Primers", y="# Sequences containing primers") +
  base_theme +
  scale_fill_gradient(low = "firebrick", high = "darkslateblue", 
                      na.value = "grey", oob = scales::squish) 

gg.primerpres <- gg.primerpresF / gg.primerpresR

gg.primerpres

pdf(file="fig/primerpres.pdf", width = 8, height = 11 , paper="a4")
  plot(gg.primerpres)
try(dev.off(), silent=TRUE)
  
```

# Overview of COI

## Entropy
```{r Entropy}
# Get whole alignment entropy
seqs <- insect::readFASTA("reference/merged_final.fa.gz")
seqs <- seqs[lengths(seqs)==712]

## Entropy by individual order
queryrank <- "order"

# Get unique querieranks
queries <- names(seqs)  %>% 
  str_split_fixed(";", n = 8) %>% 
  as_tibble() %>% 
  magrittr::set_colnames(c("acc", "kingdom", "phylum",
                           "class", "order", "family", 
                           "genus", "species")) %>%
  filter(class %in% c("Insecta", "Arachnida")) %>%
  pull(queryrank) %>%
  unique()

#Make lists to store everything
entlist <- vector("list", length=length(queries))
names(entlist) <- queries

for (i in 1:length(queries)){
  print(i)
  query <- queries[i]
 print(query)
 
 subset <- filter_by_tax(seqs, filtrank=queryrank, filtvalue=query)
  if (length(subset) > 0){
    nseqs <- length(subset)
    message(nseqs, " sequences for ", query)
    # Get entropies
    entlist[[i]] <- taxreturn::alignment_entropy(as.list(subset), maskgaps=1, countgaps=FALSE, 
                                        method="ML", unit="log", return_extra = TRUE)
  }
}

ent_out <- bind_rows(entlist, .id="names")
write_csv(ent_out, paste0("primer_evaluation/", queryrank,"_ent_out.csv"))
```

## Figure 3 
```{r Figure 3 plotting}
# Read in per order entropy
ent <- read_csv("primer_evaluation/order_ent_out.csv") %>% 
  filter(bases > 20) # Filter to only those above 20 seqs

#Set moving average function - Adjust smoothing (n=5?)
ma <- function(x, n=3){stats::filter(x, rep(1/n, n), sides=2)}

ent_smoothed <- ent %>%
  mutate(ent = ent %>% 
           na_if("") %>%
           replace_na(0)) %>%
  mutate(ma = ma(ent, n = 3)) %>%
  mutate(ma = ma %>% replace_na(0)) %>%
  mutate(annot = case_when(
    pos %in% seq(from=1, to=2, by=1) ~ "Loop 0",
    pos %in% seq(from=3, to=78, by=1) ~ "Helix 1",
    pos %in% seq(from=79, to=103, by=1)~ "Loop 1-2",
    pos %in% seq(from=104, to=211, by=1)~ "Helix 2",    
    pos %in% seq(from=212, to=235, by=1)~ "Loop 2-3",   
    pos %in% seq(from=213, to=304, by=1)~ "Helix 3", 
    pos %in% seq(from=305, to=373, by=1)~ "Loop 3-4", 
    pos %in% seq(from=374, to=466, by=1)~ "Helix 4", 
    pos %in% seq(from=467, to=499, by=1)~ "Loop 4-5", 
    pos %in% seq(from=500, to=598, by=1)~ "Helix 5", 
    pos %in% seq(from=599, to=634, by=1)~ "Loop 5-6", 
    pos %in% seq(from=635, to=712, by=1)~ "Helix 6", 
  )) %>%
  mutate(structure = case_when(
    str_detect(annot, "Helix") ~ "Helix",
    str_detect(annot, "Loop") ~ "Loop",
  ))  

## plot entropy by order
colourCount = length(unique(ent_smoothed$names))
getPalette = colorRampPalette(brewer.pal(9, "Paired"))

gg.separate_ent <- ent_smoothed %>%
  group_by(pos) %>%
  mutate(median = median(ma)) %>%
  ggplot(aes(x = pos, y=ma, colour=names)) + 
  geom_line(aes(x = pos, y=ma)) +
  facet_wrap(~names, ncol=3) +
  base_theme +
  labs(
    x="Position within COI barcode locus",
    y="Shannons Entropy (H)")  +
  scale_color_manual(values = getPalette(colourCount))+
  scale_x_continuous(limits = c(0, 712), expand=c(0,0))

gg.separate_ent
pdf(file="fig/supplementary/all_orders_entropy.pdf", width = 8, height = 11 , paper="a4")
  plot(gg.separate_ent)
try(dev.off(), silent=TRUE)


# Plot entropy of COI Gene
gg.entropy <- ent_smoothed %>%
  group_by(pos) %>%
  mutate(median = median(ma)) %>%
  ggplot(aes(x = pos, y=ma, group=pos, colour=structure)) + 
  geom_boxplot(outlier.shape = NA, alpha=0.8) +
  geom_line(aes(x = pos, y=median),size=1, inherit.aes = FALSE) + #, colour="black"
  theme_classic() +    
  theme(legend.position = "none") +
  labs(
    x="Position within COI barcode locus",
    y="Shannons Entropy (H)",
    colour="3D structure") +
  scale_color_manual(values=c("#e31a1c","#1f78b4")) +
  scale_x_continuous(limits = c(0, 712), breaks=seq(0,700,50), expand=c(0,0)) 

# Plot number of sequence across whole gene
gg.sequences <- ent_smoothed %>%
  group_by(pos) %>%
  summarise(bases = sum(bases), structure) %>%
  ggplot(aes(x = pos, y=bases)) + 
  geom_line()+
  #geom_boxplot(outlier.shape = NA, alpha=0.8) +
  theme_classic() +    
  theme(legend.position = "none") +
  labs(
    x="Position within COI barcode locus",
    y="# Sequences", colour=NULL) +
  scale_x_continuous(limits = c(0, 712), breaks=seq(0,700,50), expand=c(0,0)) +
  scale_color_manual(values=c("#e31a1c","#1f78b4")) 


## Primers
range_join <- function(x, y, value, range_start, range_stop){
  if(!any(stringr::str_detect(colnames(y), range_start)) | !any(stringr::str_detect(colnames(y), range_stop))){
    stop("Arguments range_start and range_stop must be valid columns in y")
  }
  splits <- split(y, 1:nrow(y))
  out <- tibble()
  for (i in 1:length(splits)){ 
    y_ <- splits[[i]]
    dat <- x[x[[value]] >= y_[[range_start]] & x[[value]] < y_[[range_stop]],]     
    if(nrow(dat) > 0)      
      out <-  out %>% bind_rows(dat %>% cbind(y_))  
  }
  return(out)
}

primers <- read_csv("primer_evaluation/primer_candidates.csv") %>% 
  distinct()

#Add ranks in overlapping windows to arrange y by
windowsize=130

primer_windows <- primers %>%
  range_join(
    enframe(seq(min(primers$start), max(primers$end + 100), by = windowsize)) %>%
    dplyr::rename(window= name, winend=value) %>%
    mutate(winstart = winend - windowsize), value="start", range_start="winstart", range_stop="winend") %>%
  group_by(window) %>%
  arrange(start) %>%
  mutate(rank=row_number()) %>%
  ungroup()

gg.primers <- ggplot(data=ent_smoothed[ent_smoothed$pos==1,], aes(x = as.numeric(pos))) +
  geom_segment(data = primer_windows %>% filter(strand=="F"),
               aes(x = start, xend = end,
                   y = rank, yend = rank,
                   colour=strand), size = 1, arrow = arrow(length = unit(0.3,"cm")),
               show.legend = FALSE) +
  geom_text(data = primer_windows %>% filter(strand=="F"),
            aes(x = start, y = rank,
                label = name , colour=strand),
            hjust = 1, show.legend = FALSE) +
  geom_segment(data = primer_windows %>% filter(strand=="R"),
               aes(x = end, xend = start,
                   y = rank, yend = rank,
                   colour=strand), size = 1,
               arrow = arrow(length = unit(0.3,"cm")),
               show.legend = FALSE) +
  geom_text(data = primer_windows %>% filter(strand=="R"),
            aes(x = end, y = rank,
                label = name, colour=strand),
            hjust = 0, show.legend = FALSE)  +
  scale_x_continuous(limits = c(0, 712), breaks=seq(0,700,50), expand=c(0,0)) +
  theme_void()+
    labs(x="Position within COI barcode locus", y=NULL, colour=NULL)  +
  scale_color_manual(values=c("#e31a1c","#1f78b4"))

gg.primers

Fig3 <- gg.box / gg.primers / gg.sequences + plot_layout(heights= c(3, 3, 0.5))  +  plot_annotation(tag_levels = 'A') 

Fig3

pdf(file="fig/Fig3_COI_overview.pdf", width = 11, height = 8 , paper="a4r")
  plot(Fig3)
try(dev.off(), silent=TRUE)

# Sliding window of entropy
sw <- function(x, width, interval = 1){
  win <- seq(1,  length(x) - width, by = interval) #Get all possible windows
  out <- vector("numeric", length=length(x))
  for(i in 1:length(win)){
  out[[i]] <- sum(x[win[i]:(win[i] + width)])
  }
  out[out==0] <- NA
  return(out)
}

ent_sw <- ent %>%
  group_by(names) %>%
  mutate(sw220 = sw(ent, width=220, interval = 1)/220) %>%
  mutate(sw420 = sw(ent, width=420, interval = 1)/420) %>%
  mutate(sw20 = sw(ent, width=20, interval = 1)/20) %>% #For primers
  ungroup()%>%
  pivot_longer(starts_with("sw"),
               names_to = "windowsize",
               values_to = "sw") 

gg.density <-  ent_sw %>%
  dplyr::rename(Order = names) %>%
  mutate(Order = case_when(
     Order %in% c("Coleoptera", "Diptera", "Hemiptera", "Hymenoptera", "Lepidoptera") ~ Order,
    !Order %in% c("Coleoptera", "Diptera", "Hemiptera", "Hymenoptera", "Lepidoptera") ~ "Other"
    )) %>%
  ggplot(aes(x = pos, y=1)) +
    geom_tile(aes(fill=sw))+
    scale_fill_viridis_c(option="plasma") + 
  facet_wrap(windowsize~Order, ncol=1, strip.position ="right") +
  scale_x_continuous(limits = c(0, 712), breaks=seq(0,700,50), expand=c(0,0))  +
  theme_void() +
  theme(legend.position = "bottom",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank())

gg.density
 

```

# Predicted mismatch

Should be able to multithread this with furrr

```{r Primerminer, message=FALSE}
library(PrimerMiner)
## Target sequences to test against
seqs <- insect::readFASTA("reference/merged_final.fa.gz")
names(seqs) <- names(seqs) %>% str_replace_all(" ", "_")
writeFASTA(seqs, "primer_evaluation/merged_final_target.fa")

target <- "primer_evaluation/merged_final_target.fa"

# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  mutate(seq = str_replace_all(seq, "I","N")) %>% 
  select(-score, -issues)

dir.create("primer_evaluation/PrimerMiner")

for (i in 1:nrow(primers)) {
  if(primers$strand[i]=="F"){
  evaluate_primer(target,
   as.character(primers$seq[i]), primers$start[i], primers$end[i],
   forward = TRUE, gap_NA = TRUE, N_NA=TRUE,
   mm_position = "Position_v1", mm_type = "Type_v1", adjacent = 2,
   save = paste0("primer_evaluation/PrimerMiner/", primers$name[i],".csv")
  )
  } else if(primers$strand[i]=="R"){
  evaluate_primer(target,
   as.character(primers$seq[i]), primers$start[i], primers$end[i],
   forward = FALSE, gap_NA = TRUE, N_NA=TRUE,
   mm_position = "Position_v1", mm_type = "Type_v1", adjacent = 2,
   save = paste0("primer_evaluation/PrimerMiner/", primers$name[i],".csv")
  )
 }
}

# gZip files
fs::dir_ls(path="primer_evaluation/PrimerMiner", glob = "*.csv") %>%
  purrr::map(R.utils::gzip)

```

## Figure 4 - Mismatch

```{r figure 4}
# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  mutate(seq = str_replace_all(seq, "I","N")) %>% 
  select(-score, -issues) %>%
  distinct()

#Read in all files

dat <- fs::dir_ls(path="primer_evaluation/PrimerMiner", glob = "*.csv|*.csv.gz") %>%
  purrr::set_names() %>%
  map_dfr(vroom::vroom, col_select= list("Template", "sequ", "sum"),
          .id = "source", progress=FALSE) %>%
  mutate(Template = str_remove(Template, ";$"))%>%
  separate(col=Template, into=c("Acc","Kingdom","Phylum","Class","Order","Family","Genus","Species"), sep=";") %>%
  dplyr::rename(Sequence = sequ) %>% 
  dplyr::mutate(primer = str_remove(basename(source), ".csv")) 

# set summary level
#sumlevel <- "Family"

summaries <- dat %>%
  tidyr::separate(Acc, into=c("Acc", "tax_id"), sep="\\|") %>%
  group_by(primer, Genus, tax_id) %>% #Group by higher to minimize plotting
  summarise(sum = mean(sum)) %>%
  ungroup()%>%
  mutate(dup = paste0(primer, Genus)) %>% 
  filter(!duplicated(dup)) %>% #remove any duplicates
  dplyr::select(-dup) %>%
  mutate(dir = case_when(
    primer %in% unique(primers %>% filter(strand=="F") %>% pull(name)) ~ "Forward",
    primer %in% unique(primers %>% filter(strand=="R") %>% pull(name)) ~ "Reverse"
    ))  

write_csv(summaries, "primer_evaluation/mismatch_summary.csv")
R.utils::gzip("primer_evaluation/mismatch_summary.csv")

summaries <- vroom::vroom("primer_evaluation/mismatch_summary.csv.gz", delim = ",") %>%
  dplyr::select(-dir)

# Arrange on tree
library(castor)
tree <- ape::read.tree("reference/")

# Prune tree to genus
tips_to_keep <- dat %>%
  dplyr::select(Kingdom, Phylum, Class, Order, Family, Genus, Species) %>%
  dplyr::filter(!duplicated(Species)) %>%
  dplyr::filter(Class=="Insecta") %>%
  filter(Genus %in% (summaries %>% pull(Genus))) %>%
  group_by(Genus) %>%
  dplyr::slice(1) %>%
  ungroup() %>%
  mutate(name_check = case_when(
    str_extract(Species, "^.+?(?=_)") == Genus ~ TRUE,
    !str_extract(Species, "^.+?(?=_)") == Genus ~ FALSE
  )) %>%
  filter(name_check) %>%
  dplyr::select(-name_check)

tree  <- castor::get_subtree_with_tips(tree,
                                          omit_tips=setdiff(tree$tip.label, tips_to_keep$Species),
                                          collapse_monofurcations=TRUE,
                                          force_keep_root=TRUE)$subtree

#Prune names to genus
tree$tip.label <- tree$tip.label %>% str_extract("^.+?(?=_)")

Ntips 	<- length(tree$tip.label)
Nnodes 	<- tree$Nnode
cat(sprintf("Tree has %d nodes, %d tips and %d edges\n",Nnodes,Ntips,nrow(tree$edge)));

# create internal node labels
tree$node.label <- NA
if(is.na(tree$node.label)){
	cat(sprintf("Adding node labels to full tree..\n"))
	tree$node.label = paste("node.", 1:Nnodes, sep = "") # don't use underscores, because some tree readers (e.g. rncl) interpret them as spaces
}

# replace zero-length edges
if(any(tree$edge.length==0)){
  epsilon = 0.1*min(tree$edge.length[tree$edge.length>0])
	cat(sprintf("Note: Some edges have length zero, which may break some of the HSP routines. Replacing zero-lengths with a tiny positive length (%g)..\n",epsilon))
	tree$edge.length[tree$edge.length==0] = epsilon
}


## Get values for higher nodes with castor
uprimers <- unique(summaries$primer)
p_weights <- vector("list", length=length(uprimers))
for (i in 1:length(uprimers)){
  print(paste0("Processing Primer ",i, " of ", length(uprimers), ": ", uprimers[i]))
  tip_states <- summaries %>%
    dplyr::filter(primer==uprimers[i], Genus %in% tips_to_keep$Genus) %>%
    group_by(Genus) %>%
    summarise(values = mean(sum, na.rm=TRUE))%>%
    ungroup() %>%
    column_to_rownames("Genus")
    
  row2tip <- match(rownames(tip_states), tree$tip.label)
  tip_states <- tip_states[!is.na(row2tip),,drop = FALSE]
  hsp_states <- castor::hsp_independent_contrasts(tree = tree,
                                              tip_states = tip_states$values,
                                              weighted = TRUE,
                                              check_input = TRUE)$states
  
   p_weights[[i]] <- tip_states %>%
    rownames_to_column("Genus") %>%
    mutate(hsp = hsp_states[1:Ntips])
}
names(p_weights) <- uprimers

imputed <- summaries %>%
    left_join(bind_rows(p_weights,.id = "primer"), 
               by = c("primer", "Genus")) %>%
  filter(!is.na(hsp))

primer_orders <- imputed %>%
  group_by(primer) %>%
  dplyr::summarise(hsp = mean(hsp, na.rm=TRUE), sum = mean(sum, na.rm=TRUE)) %>%
  arrange(hsp)

# LAbel branches
tax_groups <- tips_to_keep %>%
dplyr::rename(label = Genus) %>%
select(label, Order) %>%
filter(label %in% tree$tip.label) %>%
group_by(Order)

group_name <- group_keys(tax_groups)  %>%
mutate(group_name = Order %>% str_remove_all("\\[|\\]"))

cls <- tax_groups %>%
group_split() %>%
purrr::map(pull, label) %>%
set_names(group_name$group_name)

tree2 <- groupOTU(tree, cls)

# Plot tree
p1 <- ggtree(tree2, ladderize=TRUE, aes(colour=values)) +
  geom_text2(aes(subset=!isTip, label=group %>% na_if(0)), hjust=0, check_overlap=TRUE)

weights_p1 <- p1$data %>%
  left_join(imputed %>%
              group_by(Genus) %>%
              summarise(values = mean(sum, na.rm=TRUE))%>%
              dplyr::rename(label = Genus)
    )  


tip_states <- weights_p1 %>%
  dplyr::filter(isTip) %>%
  pull(values)
names(tip_states) <- weights_p1 %>%
  dplyr::filter(isTip) %>%
  pull(label)

weights_p1$values[!weights_p1$isTip] <- castor::asr_independent_contrasts(tree=tree,
                                                        tip_states=tip_states)$ancestral_states

p2 <- p1 %<+% weights_p1 + geom_tippoint(aes(colour=values)) +
  scale_color_gradient(low="darkslateblue", high="firebrick") +
  theme(legend.position = "none") + 
  scale_y_continuous(expand=c(0,0))+ 
  scale_x_discrete(expand=c(0,0)) 

## make a clade label list
#tax_groups <- tips_to_keep %>%
#  dplyr::rename(label = Genus) %>%
#  select(label, Order) %>%
#  filter(label %in% tree$tip.label) %>%
#  group_by(Order) 
#
#group_name <- group_keys(tax_groups)  %>%
#  mutate(group_name = Order %>% str_remove_all("\\[|\\]"))
#
#cls <- tax_groups %>% 
#  group_split() %>% 
#  purrr::map(pull, label) %>%
#  set_names(group_name$group_name) %>%
#  purrr::map(get_mrca_of_set, tree=tree)
#
#p3 <- p2
#for(i in 1:length(cls)){
#p3 <- p3 + geom_cladelabel(node=cls[[i]], label=names(cls[i]), align=T, angle=270, hjust='center', #offset.text=.5, barsize=1.5)
#}
# Plot heatmap
gg.mismatch <- imputed %>%
  left_join(p2$data %>% dplyr::select(label, y) %>% dplyr::rename(Genus = label)) %>%
  mutate( primer = factor(primer, levels=primer_orders$primer)) %>%     
  ggplot(aes(x = primer, y = y, fill = hsp)) +
    geom_raster() +
    #scale_fill_distiller(palette = "Spectral", limits = c(0, 200)) +
    scale_fill_gradient(low = "darkslateblue", high = "firebrick", na.value = "grey", limits = c(0, 200), oob = scales::squish) +
    facet_grid(~dir, scales="free", space="free", drop=TRUE) +
    #theme_classic() + 
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.title.y = element_blank(),
      strip.text.y = element_text(angle = 0),
      #strip.background = element_rect(fill="grey10"),
      legend.position = "none"
    ) + 
  scale_y_continuous(expand=c(0,0))+ 
  scale_x_discrete(expand=c(0,0))

# Density plot of mismatch
gg.density <- imputed %>%
  left_join(p2$data %>% dplyr::select(label, y) %>% dplyr::rename(Genus = label)) %>%
  group_by(Genus, y) %>%
  summarise(values = mean(sum, na.rm=TRUE)) %>%
  ggplot(aes(x = y, y=values, fill=values, colour=values)) +
  geom_point(size=0.01, alpha=0.5)+
  #geom_smooth(span = 0.1)+
  scale_colour_gradient(low="darkslateblue", high="firebrick") + 
  scale_x_continuous(expand=c(0,0)) +
  scale_y_continuous(expand=c(0,0)) +
  theme_void() +
  theme(legend.position = "none")+
  coord_flip()

#Fig3 <- p2 + gg.mismatch + plot_layout(widths = c(1,2))

Fig3 <- p2 + gg.mismatch + gg.density + plot_layout(widths = c(1,3,0.25))

pdf(file="fig/Fig3.pdf", width = 8, height = 11 , paper="a4")
  plot(Fig3)
try(dev.off(), silent=TRUE)
 

  
#- Add a summary metric for mean mismatch for each primer
#- Add a summary metric for evennes of mismatch for each primer
#- Add a summary metric for amount of missing data imputed. Is there a way to highlight what data was imputed or not?
#- Add a metric for amount of degeneracy in each primer
```


# Identification sucess

In order to evaluate the taxonomic resolution of different barcode regions, alignments were trimmed to primer regions using the in silico PCR function from the insect package and  the identificaiton sucess functions from the SPIDER r package were used

Metrics to evaluate primers on:
 *  OTU clustering of each primer at different ranks, see drop off
 *  OTU clustering, subset to pests only (ie fraction of pests in their own cluster)
 *  Datasets subset to pest genera (or families), evaluated with spider nearNeighbour, BestCloseMatch, ThreshID, Monophyly
 * Leave one out BLAST/RDP?
 * P(LCR) for each primer?
 
 
## Virtual PCR

```{r Virtual PCR}
# Load seqs
seqs <- readDNAStringSet("reference/merged_final.fa.gz")
seqs <- seqs[lengths(seqs)==712]

# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  mutate(seq = str_replace_all(seq, "I","N")) %>% 
  select(-score, -issues)

# Get all possible primer combinations
combos <- expand_grid(primers %>% filter(strand=="F") %>% pull(seq),
                      primers %>% filter(strand=="R") %>% pull(seq)
                      ) %>%
  mutate_if(is.factor, as.character)%>%
  unique() %>%
  magrittr::set_colnames(c("Fseq", "Rseq")) %>%
  left_join(primers %>% filter(strand=="F") %>% rename_all(. %>% paste0("F",.)), by="Fseq") %>%
  left_join(primers %>% filter(strand=="R") %>% rename_all(. %>% paste0("R",.)), by="Rseq") %>%
  mutate(amplicon = Rstart - Fend) %>%
  filter(amplicon > 50)

dir.create("primer_evaluation/amplicons")

# Setup multithreading
library(foreach)
library(doParallel)
cores=12
cl <- parallel::makeCluster(cores)
registerDoParallel(cl)

li <- foreach(p=1:nrow(combos)) %dopar% {
  #get primer names
  primernames <- paste0(combos$Fname[p], "_",  combos$Rname[p])
  
  # cut down alignments
  amplicon <- Biostrings::subseq(seqs, start=combos$Fend[p]+1, end = combos$Rstart[p]) #may need to add 1 to start
  
  if (any(!lengths(amplicon)== combos$amplicon[p])){
    warning(paste0("Amplicons of ",primernames, " are not the same length"))
  }
  
  maxgaps <- 9 # dont allow any more than 9 gaps
  rem <- names(amplicon)[Biostrings::letterFrequency(amplicon, "-") > maxgaps]
  amplicon <- amplicon[!names(amplicon) %in% rem]
  message(paste0(length(rem), " Sequences with more than ", maxgaps, " gaps removed from ", primernames))
  
  #write out sequences
  Biostrings::writeXStringSet(amplicon, file=paste0("primer_evaluation/amplicons/", primernames,".fa.gz"), compress=TRUE)
}
#close cluster
parallel::stopCluster(cl)

```
 
## Pairwise ID Success

Submit array

njobs=$(cat job_index | wc -l)
sbatch --array=1-808 submit_clustering.slurm

### Creat USEARCH distance matrix
```{bash generate job index}
#!/bin/bash
dos2unix bash/usearch_distmat.sh
/usr/bin/ls -d /group/pathogens/Alexp/Metabarcoding/primer_evaluation/primer_evaluation/amplicons/* | sed -e '1p' -e '/.fa.gz/!d' | sort -u > job_index.txt
njobs=$(cat job_index.txt | wc -l)
sbatch --array=1-$njobs bash/usearch_distmat.sh
```

From this distance matrix...
Can calculate the probability of lowest common ancestor at different similarities

## Get mixed clusters

```{r analyse uclust}
# Summarise number of mixed clusters that contain pests at %
pestlist <- read_csv("primer_evaluation/pestlist.csv") 

# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  mutate(seq = str_replace_all(seq, "I","N"))  %>%
  distinct()

# Get all possible primer combinations
combos <- expand_grid(primers %>% filter(strand=="F") %>% pull(seq),
                      primers %>% filter(strand=="R") %>% pull(seq)
                      ) %>%
  mutate_if(is.factor, as.character)%>%
  unique() %>%
  magrittr::set_colnames(c("Fseq", "Rseq")) %>%
  left_join(primers %>% filter(strand=="F") %>% rename_all(. %>% paste0("F",.)), by="Fseq") %>%
  left_join(primers %>% filter(strand=="R") %>% rename_all(. %>% paste0("R",.)), by="Rseq") %>%
  mutate(amplicon = Rstart - Fend) %>%
  filter( amplicon > 100)

# Check missing
complete <- list.files(path="primer_evaluation/amplicons/usearch_output/", pattern = ".txt.gz") %>%
  basename %>%
  str_remove(".txt.gz")

setdiff(paste0(combos$Fname, "_",  combos$Rname), complete)
setdiff(complete,paste0(combos$Fname, "_",  combos$Rname))


# Get mixed clusters
library(furrr)
plan(multiprocess, workers=4)
mixed_clusters <- fs::dir_ls(path="primer_evaluation/amplicons/usearch_output/", glob = "*.txt.gz")%>%
  furrr::future_map(function(x){
  #purrr::map(function(x){
  df <- vroom::vroom(x, delim="\t", col_names = c("acc1", "acc2", "dist")) %>%
    dplyr::mutate(spp1 = acc1 %>%
                    str_remove(";$") %>%
                    str_remove("^.*;"),
                  acc1 = acc1 %>% str_remove(";.*$"),
                  spp2 = acc2 %>%
                    str_remove(";$") %>%
                    str_remove("^.*;"),
                  acc2 = acc2 %>% str_remove(";.*$"),
                  dist = round(dist, 2)) %>%
    dplyr::filter(!spp1==spp2)

  all <- df %>%
        dplyr::group_by(dist) %>% 
        dplyr::summarise(mixed=n_distinct(spp1)) %>%
        dplyr::mutate(type="all")
  pests <- df %>%
        dplyr::group_by(dist) %>% 
        filter(spp1 %in% pestlist$Species) %>%
        dplyr::summarise(mixed=n_distinct(spp1)) %>%
        dplyr::mutate(type="pest")
      out <- bind_rows(all, pests)
      return(out)
  
  })%>%
  bind_rows(.id="source") %>%
  mutate(source = str_remove(basename(source), ".txt.gz"))
  
#Get amplified
amplified <- fs::dir_ls(path="primer_evaluation/amplicons/", glob = "*.fa.gz") %>%
   purrr::map(function(x){
   #furrr::future_map(function(x){
     df <- fasta.index(x) %>%
        mutate(species = desc %>% 
                 str_remove("(?:[^;]*;){7}") %>% #match 7th ;
                 str_remove(";$")) # match last ;
     all <- df %>%
      dplyr::group_by(filepath) %>%
      dplyr::summarise(seqs_amplified = n(), spp_amplified = n_distinct(species)) %>%
      dplyr::mutate(type="all")
     pests <- df %>%
      dplyr::group_by(filepath) %>%
      dplyr::filter(species %in% pestlist$Species) %>%
      dplyr::summarise(seqs_amplified = n(), spp_amplified = n_distinct(species)) %>%
      dplyr::mutate(type="pest")
     out <- bind_rows(all, pests)
    return(out)
   }) %>%
  bind_rows() %>%
  mutate(source = str_remove(basename(filepath), ".fa.gz")) %>%
  dplyr::select(-filepath)

joint <- mixed_clusters %>%
  left_join(amplified) %>%
  mutate(combos = source) %>% 
  tidyr::separate(source, into=c("Fname", "Rname"), sep="_", extra="merge")
vroom::vroom_write(joint, "primer_evaluation/amplicons/mixed_clusters_summary.csv", delim=",")


## Plot


# Count reads
joint <- vroom::vroom("primer_evaluation/amplicons/mixed_clusters_summary.csv", delim=",") %>%
  left_join(combos) %>% 
  mutate(success =  (spp_amplified - mixed)/spp_amplified,
         threshold = 1-dist) %>%
  dplyr::select(Fname,Rname, dist, threshold,
                mixed, type, seqs_amplified,
                spp_amplified, amplicon, combos, success) 



# Look at log ratio to the folmer region
ref <- joint %>% 
  dplyr::filter(Fname == "LCO1490"&  Rname=="HCO2198") %>% 
              select(ref_success = success,
                     dist, threshold, type) %>%
  dplyr::filter(threshold %in% c(0.97,0.98,0.99, 1))%>%
  group_by(type) %>%
  summarise(ref_success = mean(ref_success)) %>%
  ungroup()

id_success <- joint %>% 
  dplyr::filter(threshold %in% c(0.97,0.98,0.99, 1))%>%
  group_by(Fname, Rname, type, combos, amplicon, threshold) %>%
  summarise(success = mean(success)) %>%
  ungroup() %>%
  left_join(ref) %>%
  mutate(success_alr = log(success/ref_success),
         success_rat = success/ref_success,
         highlight = case_when(
           combos %in% c("fwhF2_fwhR2n", "fwhF2_HexCOIR4") ~ TRUE,
           TRUE ~ FALSE
         ))

gg.id <- id_success %>%
  ggplot(aes(x=amplicon, y=success_alr, colour=success_alr))+
  geom_jitter(width = 5, height=0, alpha=0.5)+
  geom_smooth(colour="black", method="lm")+
  #geom_vline(xintercept = 220, colour="grey80") +
  #geom_vline(xintercept = 420, colour="grey80")+
  scale_colour_gradient(low ="firebrick", high ="darkslateblue", na.value = "grey") +
  geom_hline(yintercept=0, colour="grey80")+
  base_theme+
  facet_grid(threshold~type) +
  theme(legend.position = "none") + 
  labs(x="Amplicon length",
       y="Log(sucessful ID / LCO-HCO)") 

gg.id


## MODEL
# see whats signfiicant 
model_dat <- id_success %>%
  dplyr::select(combos, amplicon, threshold, success_alr) 

library(tidymodels)
lm_mod <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")  %>%
  step_normalize(all_predictors())

lm_fit <- 
  lm_mod %>% 
  fit(success_alr ~ 0 + amplicon + threshold, data = model_dat)

# Group by theshold?
tidy(lm_fit)


# Write out id
pdf(file="fig/id_success.pdf", width = 8, height = 11 , paper="a4")
  plot(gg.id)
try(dev.off(), silent=TRUE)

library(plotly)
ggplotly(gg.id)

library(tidytext)

# Barchart
id_success %>%
  filter(threshold=="0.97") %>%
 # group_by(threshold,type) %>%
  #top_frac(.1, success_alr) %>% 
  #ungroup() %>%
  ggplot(aes(x=tidytext::reorder_within(combos, success_alr, threshold), y=success_alr, fill=combos))+
  geom_col() + 
  facet_grid(threshold~type, drop=TRUE, scales="free") +
  scale_x_reordered() +
  coord_flip() +
  theme(axis.text.x = element_text(angle=45, hjust=1),
        legend.position = "none")+
  base_theme +
  geom_rangeframe(colour="black")+
  labs(x="Amplicon length",
       y="Log(sucessful ID / LCO-HCO)")

# Can i take the mean of the different threshold values?



```

## ID success on tree

for this we will analyse just the LCO HCO data. Will need to take in the raw amplicons data

Arrange this on the tree and see how much phylogenetic signal there is

The phylogenetic autocorrelation function (ACF) of known 16S GCNs across the SILVA-derived tree (Fig. 1a) was calculated using the castor function get_trait_acf based on 108 tip pairs (options âNpairs=1e8, Nbins=100â), chosen randomly among tips with known GCN. The function get_trait_acf randomly picks OTU pairs on the tree, bins them into one of many intervals of phylogenetic distance, and calculates the Pearson autocorrelation between GCNs of the OTU pairs within each bin. Note that this analysis does not assume that GCNs scale linearly with phylogenetic distance. Instead, the ACF merely measures the statistical correlation between GCNs on distinct tips, conditional upon the tips being within a certain phylogenetic distance from each other.

```{r ID success on tree}
# Arrange on tree
library(castor)
tree <- rncl::read_newick_phylo("reference/ultrametric_insecta_tree.nwk")

# Prune tree to genus
tips_to_keep <- dat %>%
  dplyr::select(Kingdom, Phylum, Class, Order, Family, Genus, Species) %>%
  dplyr::filter(!duplicated(Species)) %>%
  dplyr::filter(Class=="Insecta") %>%
  filter(Genus %in% (summaries %>% pull(Genus))) %>%
  group_by(Genus) %>%
  dplyr::slice(1) %>%
  ungroup() %>%
  mutate(name_check = case_when(
    str_extract(Species, "^.+?(?=_)") == Genus ~ TRUE,
    !str_extract(Species, "^.+?(?=_)") == Genus ~ FALSE
  )) %>%
  filter(name_check) %>%
  dplyr::select(-name_check)

tree  <- castor::get_subtree_with_tips(tree,
                                          omit_tips=setdiff(tree$tip.label, tips_to_keep$Species),
                                          collapse_monofurcations=TRUE,
                                          force_keep_root=TRUE)$subtree

#Prune names to genus
tree$tip.label <- tree$tip.label %>% str_extract("^.+?(?=_)")

Ntips 	<- length(tree$tip.label)
Nnodes 	<- tree$Nnode
cat(sprintf("Tree has %d nodes, %d tips and %d edges\n",Nnodes,Ntips,nrow(tree$edge)));

# create internal node labels
tree$node.label <- NA
if(is.na(tree$node.label)){
	cat(sprintf("Adding node labels to full tree..\n"))
	tree$node.label = paste("node.", 1:Nnodes, sep = "") # don't use underscores, because some tree readers (e.g. rncl) interpret them as spaces
}

# replace zero-length edges
if(any(tree$edge.length==0)){
  epsilon = 0.1*min(tree$edge.length[tree$edge.length>0])
	cat(sprintf("Note: Some edges have length zero, which may break some of the HSP routines. Replacing zero-lengths with a tiny positive length (%g)..\n",epsilon))
	tree$edge.length[tree$edge.length==0] = epsilon
}


## Get values for higher nodes with castor
uprimers <- unique(summaries$primer)
p_weights <- vector("list", length=length(uprimers))
for (i in 1:length(uprimers)){
  print(paste0("Processing Primer ",i, " of ", length(uprimers), ": ", uprimers[i]))
  tip_states <- summaries %>%
    dplyr::filter(primer==uprimers[i], Genus %in% tips_to_keep$Genus) %>%
    group_by(Genus) %>%
    summarise(values = mean(sum, na.rm=TRUE))%>%
    ungroup() %>%
    column_to_rownames("Genus")
    
  row2tip <- match(rownames(tip_states), tree$tip.label)
  tip_states <- tip_states[!is.na(row2tip),,drop = FALSE]
  hsp_states <- castor::hsp_independent_contrasts(tree = tree,
                                              tip_states = tip_states$values,
                                              weighted = TRUE,
                                              check_input = TRUE)$states
  
   p_weights[[i]] <- tip_states %>%
    rownames_to_column("Genus") %>%
    mutate(hsp = hsp_states[1:Ntips])
}
names(p_weights) <- uprimers

imputed <- summaries %>%
    left_join(bind_rows(p_weights,.id = "primer"), 
               by = c("primer", "Genus")) %>%
  filter(!is.na(hsp))

primer_orders <- imputed %>%
  group_by(primer) %>%
  dplyr::summarise(hsp = mean(hsp, na.rm=TRUE), sum = mean(sum, na.rm=TRUE)) %>%
  arrange(hsp)

# LAbel branches
tax_groups <- tips_to_keep %>%
dplyr::rename(label = Genus) %>%
select(label, Order) %>%
filter(label %in% tree$tip.label) %>%
group_by(Order)

group_name <- group_keys(tax_groups)  %>%
mutate(group_name = Order %>% str_remove_all("\\[|\\]"))

cls <- tax_groups %>%
group_split() %>%
purrr::map(pull, label) %>%
set_names(group_name$group_name)

tree2 <- groupOTU(tree, cls)

# Plot tree
p1 <- ggtree(tree2, ladderize=TRUE, aes(colour=values)) +
  geom_text2(aes(subset=!isTip, label=group %>% na_if(0)), hjust=0, check_overlap=TRUE)

weights_p1 <- p1$data %>%
  left_join(imputed %>%
              group_by(Genus) %>%
              summarise(values = mean(sum, na.rm=TRUE))%>%
              dplyr::rename(label = Genus)
    )  


tip_states <- weights_p1 %>%
  dplyr::filter(isTip) %>%
  pull(values)
names(tip_states) <- weights_p1 %>%
  dplyr::filter(isTip) %>%
  pull(label)

weights_p1$values[!weights_p1$isTip] <- castor::asr_independent_contrasts(tree=tree,
                                                        tip_states=tip_states)$ancestral_states

p2 <- p1 %<+% weights_p1 + geom_tippoint(aes(colour=values)) +
  scale_color_gradient(low="darkslateblue", high="firebrick") +
  theme(legend.position = "none") + 
  scale_y_continuous(expand=c(0,0))+ 
  scale_x_discrete(expand=c(0,0)) 


###################################
# AUTOCORRELATION FUNCTION OF TRAIT

if(INCLUDE_ACF){
	cat(sprintf("Calculating ACF and MRD of trait '%s'..\n",trait_name))
	acf_results = castor::get_trait_acf(known_tree, known_tip_states, Npairs=1e8, Nbins=100)

	# save stats to file
	cat(sprintf("Saving autocorrelation function to TSV file..\n"))
	output_path = sprintf("output/ACF.tsv")
	check_output_file(output_path,TRUE,TRUE,"  ");						
	cat(sprintf("# Phylogenetic autocorrelation function (ACF) and mean absolute difference (MAD), of '%s' across tips\n# ACF(x) = correlation between the states of two random tips at distance x from each other\n# distance\tautocorrelation\tmean_absolute_deviation\tmean_relative_deviation\tNpairs\n",trait_name), file=output_path, append=FALSE);
	write.table(x=data.frame(acf_results$distances, acf_results$autocorrelations, acf_results$mean_abs_differences, acf_results$mean_rel_differences, acf_results$Npairs_per_distance), file=output_path, append=TRUE, sep="\t", row.names=FALSE, col.names=FALSE, quote = FALSE);

	# plot ACF
	cat(sprintf("Plotting ACF..\n"))
	plot_file=sprintf("output/ACF.pdf")
	check_output_file(plot_file,TRUE,TRUE,"  ")
	pdf(file=plot_file, width=DEFAULT_PLOT_WIDTH, height=DEFAULT_PLOT_HEIGHT);
	par(las=1);
	plot(x=100*acf_results$distances, y=acf_results$autocorrelations, type="l", col="black", pch=1, cex=0.6, las=1, lwd=1.5, xlab="phylogenetic distance (% substitutions per site)", ylab="autocorrelation", xlim=c(0,80), ylim=c(-0.1,max(acf_results$autocorrelations[acf_results$distances<=1],na.rm=TRUE)));
	invisible(dev.off());

	# plot MAD
	cat(sprintf("Plotting MAD..\n"))
	plot_file=sprintf("output/mean_abs_differences.pdf")
	check_output_file(plot_file,TRUE,TRUE,"  ")
	pdf(file=plot_file, width=DEFAULT_PLOT_WIDTH, height=DEFAULT_PLOT_HEIGHT);
	par(las=1);
	plot(x=100*acf_results$distances, y=acf_results$mean_abs_differences, type="l", col="black", pch=1, cex=0.6, las=1, lwd=1.5, xlab="phylogenetic distance (% substitutions per site)", ylab="mean abs difference", xlim=c(0,80), ylim=c(-0.1,max(acf_results$mean_abs_differences[acf_results$distances<=1],na.rm=TRUE)));
	invisible(dev.off());
	
	# plot MRD
	cat(sprintf("Plotting MRD..\n"))
	plot_file=sprintf("output/mean_rel_differences.pdf")
	check_output_file(plot_file,TRUE,TRUE,"  ")
	pdf(file=plot_file, width=DEFAULT_PLOT_WIDTH, height=DEFAULT_PLOT_HEIGHT);
	par(las=1);
	plot(x=100*acf_results$distances, y=acf_results$mean_rel_differences, type="l", col="black", pch=1, cex=0.6, las=1, lwd=1.5, xlab="phylogenetic distance (% substitutions per site)", ylab="mean rel difference", xlim=c(0,80), ylim=c(-0.1,max(acf_results$mean_rel_differences[acf_results$distances<=1],na.rm=TRUE)));
	invisible(dev.off());
}
```


## Get LCA Probabilities

```{r lca probs}
## Get LCA probs
library(furrr)
plan(multiprocess)

ranks = c("kingdom", 
    "phylum", "class", "order", "family", 
    "genus", "species")

lca_probs <- fs::dir_ls(path="primer_evaluation/amplicons/uclust_output/", glob = "*.txt.gz") %>%
  furrr::future_map(function(x){
  #purrr::map(function(x){
    print(x)
  df <- vroom::vroom(x, delim="\t", col_names = c("acc1", "acc2", "dist")) %>%
    dplyr::mutate(acc1 = acc1 %>%
                    str_remove(";$"),
                  acc2 = acc2 %>%
                    str_remove(";$"),
      dist = round(dist, 2),
                  ) 
  lca <- df %>%
    group_by(dist) %>%
    group_split() %>%
    purrr::map(function(y){
      print(unique(y$dist))
      logidf <- y %>% 
        tidyr::separate(acc1, into = c("Acc", paste0(ranks, "_1")), sep = ";") %>%
        tidyr::separate(acc2, into = c("Acc", paste0(ranks, "_2")), sep = ";") %>%
        transmute(
          kingdom = case_when(kingdom_1 == kingdom_2 ~ TRUE, TRUE ~ FALSE),
          phylum = case_when(phylum_1 == phylum_2 ~ TRUE, TRUE ~ FALSE),
          class = case_when(class_1 == class_2 ~ TRUE, TRUE ~ FALSE),
          order = case_when(order_1 == order_2 ~ TRUE, TRUE ~ FALSE),
          family = case_when(family_1 == family_2 ~ TRUE, TRUE ~ FALSE),
          genus = case_when(genus_1 == genus_2 ~ TRUE, TRUE ~ FALSE),
          species = case_when(species_1 == species_2 ~ TRUE, TRUE ~ FALSE),
          ) %>%
        dplyr::select(rev(ranks)) %>%
        as.data.frame()
        keepvec <- apply(logidf, 1, which.max)
        rows <- seq(as.data.frame(logidf)[, 1])
        selector <- matrix(ncol = 2, c(rows, keepvec))
        logidf[selector] <- "KEEP"
        logidf[!logidf == "KEEP"] <- 0
        logidf[logidf == "KEEP"] <- 1
        logidf <- logidf %>%
          mutate_all(as.numeric) %>%
          colSums()/length(rows) 
        out <-as_tibble(logidf, rownames="rank") %>%
          mutate(dist = unique(y$dist))
        return(out)
    }) %>%
  dplyr::bind_rows() %>% 
      dplyr::group_by(rank, dist) %>% 
      dplyr::summarise(prob = mean(value))
  
  return(lca)
  }) %>%
bind_rows(.id="source") %>%
mutate(source = str_remove(basename(source), ".txt.gz"))

#Write out lca probabilities
vroom::vroom_write(lca_probs, "primer_evaluation/lca_probs.csv")


# read in lca probabilities
lca <- vroom::vroom("primer_evaluation/lca_probs.csv") %>%
  dplyr::rename(name = source)

# Plot out
gg.lca <- lca %>%
  dplyr::filter(rank %in% c("species", "genus", "family", "order")) %>%
  mutate(rank = factor(rank, levels = c("species", "genus", "family", "order"))) %>%
  ggplot(aes(x=dist, y=prob, colour = name)) +
  geom_point() +
  base_theme +
  facet_wrap(~rank, ncol=1)

gg.lca

pdf(file="fig/lca_probabilities.pdf", width = 8, height = 11 , paper="a4")
  plot(gg.lca)
try(dev.off(), silent=TRUE)
 
``` 
 

# Off-target amplification

First identify all sequences that have homology to both forward and reverse primers, with the hit sequences placed so that they can actually form a PCR product. 
First perform a blast search for each primer individually against NT database (or can you do a kmer search with BBDUK?)
Get accession numbers, get taxonomy for accession numbers
then compare the lists for forward and reverse primers.
Any accession number that occurs in both lists needs to be investigated as a potential cross-reacting sequence
Get taxonomy for all cross reactign sequences, filter to those that are not insecta
Retrieve all sequences for cross reacting, make new database from those
Then do in-silico PCR with insect and count how many successfully amplify

```{r off target}
dir.create("primer_evaluation/off_target/individual", recursive = TRUE)
# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  dplyr::filter(!duplicated(name)) %>%
  mutate(seq = str_replace_all(seq, "I","N")) %>% 
  mutate(seq =case_when(
    strand=="F" ~ seq,
    strand=="R" ~ rc(seq) #Works without RC but RC seems faster
  )) 

# Disambiguate primers and write out individuals
seqs <- DECIPHER::Disambiguate(DNAStringSet(primers$seq))
names(seqs) <- primers$name

for (i in 1:length(seqs)){
  out <- unlist(seqs[i])
  #Sample 100 random permutations for degenerate primers
  out <- sample(out, 100, replace = TRUE) %>%
    unique()
  names(out) <- make.unique(names(out), sep="_")
  writeXStringSet(out, paste0("primer_evaluation/off_target/individual/",names(out[1]),".fa"))
}


dir.create("primer_evaluation/off_target/joined", recursive = TRUE)
# Get all possible primer combinations
combos <- expand_grid(primers %>% filter(strand=="F") %>% pull(seq),
                      primers %>% filter(strand=="R") %>% pull(seq)
                      ) %>%
  mutate_if(is.factor, as.character)%>%
  unique() %>%
  magrittr::set_colnames(c("Fseq", "Rseq")) %>%
  left_join(primers %>% filter(strand=="F") %>% rename_all(. %>% paste0("F",.)), by="Fseq") %>%
  left_join(primers %>% filter(strand=="R") %>% rename_all(. %>% paste0("R",.)), by="Rseq") %>%
  mutate(amplicon = Rstart - Fend) %>%
  filter( amplicon > 100) %>%
  mutate(synthetic = paste0(Fseq,"-", rc(Rseq) )) 

# Write out all combos - takes a while due to extremely high degeneracy!
for(i in 1:nrow(combos)){
  print(i)
  out <- DECIPHER::Disambiguate(DNAStringSet(combos$synthetic[i]))
  names(out) <- paste0(combos$Fname[i], "_",  combos$Rname[i])
  out <- unlist(out)
  # Pad with 20 N's
  out <- DNAStringSet(sapply(out, str_replace_all, pattern= "-", replacement="NNNNNNNNNNNNNNNNNNNN"))
  
  names(out) <- make.unique(names(out), sep="_")
  writeXStringSet(out, paste0("primer_evaluation/off_target/joined/", names(out[1]),".fa"))
}

# For the combos should i just take a random sample of 1000? as this is pretty ridiculous

```

# Off target chunked

Output all primer combos, fasta files in chunks of 100
Do as a BLAST array job
pull the taxid, and query name
get only unique taxid
save that as the results
Calculate off targets as proportion of insecta to total sequences amplified

Probably dont need to do it for all combos? just do for forward and reverse. Define it as a limitation 

```{r off target}
dir.create("primer_evaluation/off_target/individual_chunked", recursive = TRUE)
# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  dplyr::filter(!duplicated(name)) %>%
  mutate(seq = str_replace_all(seq, "I","N")) %>% 
  mutate(seq =case_when(
    strand=="F" ~ seq,
    strand=="R" ~ rc(seq) #Works without RC but RC seems faster
  )) 

# Disambiguate primers and write out individuals
seqs <- DECIPHER::Disambiguate(DNAStringSet(primers$seq))
names(seqs) <- primers$name

for (i in 1:length(seqs)){
  out <- unlist(seqs[i])
  names(out) <- make.unique(names(out), sep="_") 
  # Split into chunks of 100
  chunks <- split(out, ceiling(seq_along(out)/100))

  #write out chunks as seperate files
  for (c in 1:length(chunks)){
    filename <- paste0(names(seqs[i]),"_", names(chunks[c]))
    writeXStringSet(chunks[[c]],
                    paste0("primer_evaluation/off_target/individual_chunked/",filename,".fa.gz"),
                    compress = TRUE)
  }
}
```


See primerblast paper:

To evaluate specificity, artificial search sequences were generated by concatenating both primer sequences with a 20 base spacer. This ensures that each primer will be treated separately in the BLAST search and thus achieves the equivalent effect of performing a separate BLAST search for each primer. To create a database of potential non-target sequence ampliciations These artificial sequences as well as just the forward and reverse primers were searched against the local NCBI nr database using BLASTn

From primerserver code : https://github.com/billzt/PrimerServer/blob/master/script/_run_specificity_check.pl
blastn -task blastn-short -query $query_file -db $db_file -evalue 30000 "
                    ." -word_size 7 -perc_identity60 -dust no -ungapped -reward 1 -penalty -1 "
                    ." -max_hsps 500 -outfmt '6 qseqid qstart qend sseqid sstart send sstrand' "
                    ." -out $query_file.$db_name.out -num_threads $run_cpu";


Index jobs
```{bash generate job index}
#!/bin/bash
/usr/bin/ls -d $PWD/*.fa.gz | sort -u > sequence_index.txt
```

Submit array

njobs=$(cat sequence_index.txt | wc -l)

sbatch --array=1-930 primerblast.slurm

sbatch --array=1-4 primerblast.slurm

## BLAST
```{bash blast}
#!/bin/bash
#SBATCH --job-name=BLASTn       
#SBATCH --ntasks=1 
#SBATCH --cpus-per-task=4
#SBATCH --mem=20GB
#SBATCH --time=240:00:00
#SBATCH --mail-user=alexander.piper@agriculture.vic.gov.au
#SBATCH --mail-type=ALL
#SBATCH --account=pathogens
#SBATCH --export=none

#Check if job is launched as array
if [ -z "$SLURM_ARRAY_TASK_COUNT" ]; then 
  echo SLURM_ARRAY_TASK_COUNT unset; 
  echo You must launch this job as an array
  echo see https://slurm.schedmd.com/job_array.html
  echo for info on how to run arrays
  exit 1
fi
Index=sequence_index.txt

# Make sure that sequence index file is there before we do anything
if [[ ! -f "${Index}" ]]; then
  echo "Error sequence index file ${Index} does not exist"
  exit 1
fi

#Gather info on our samples
BlastDB=/group/blastdb/nt
FullSampleName=$(sed -n ${SLURM_ARRAY_TASK_ID}p ${Index})
SequencePath=$(dirname ${FullSampleName})
Sample=$(basename ${FullSampleName} .fa.gz)

# Double check that array index is valid
if [[ ! -f "${FullSampleName}" ]]; then
  echo "Error array index doesnt match up with index file"
  echo "Array index is  ${SLURM_ARRAY_TASK_ID}"
  exit 1
fi

# Goto tmp to do our processing
echo $TMPDIR
cd $TMPDIR
tmp_dir=$(mktemp -d -t ci-XXXXXXXXXX)
cd $tmp_dir

#Copy data files, database and decompress all
cp ${FullSampleName} .
pigz -p8 -d ./*.gz
pwd
ls

#Load modules
module purge
module load BLAST+

###START###

echo ${BlastDB}
echo ${Sample}
date

blastn -task blastn-short \
-query ${Sample}.fa \
-db  ${BlastDB} \
-out ${Sample}.out \
-evalue 30000 \
-word_size 7 \
-perc_identity 60 \
-dust no \
-ungapped \
-reward 1 \
-penalty -1 \
-max_hsps 1 \
-max_target_seqs 1000000 \
-outfmt '6 qseqid qstart qend sseqid staxid sstart send sstrand' \
-num_threads 8

pigz ${Sample}.out

# Output useful job stats
/usr/local/bin/showJobStats.scr | gzip > ${Sample}-jobstats.gz

#Make a directory call ${Sample} and cp all output files into that directory
mkdir ${Sample}_output
cp ./*.gz ${Sample}_output

date

# put all output files back where we started
cp -r ${Sample}_output ${SLURM_SUBMIT_DIR}
```

## Extract fasta of all unique BLAST hits
```{bash}
#echo $() >  hits.txt
#for d in ./*_output/ ; do (cd "$d" && zcat *.out.gz | awk '{print $4}' | awk '{split($0,a,"|"); print a[2]}' | sort -u #>> ../hits.txt ); done
#cat hits.txt | sort -u > merged_hits.txt
#
##blastdbcmd -entry_batch merged_hits.txt -db /group/blastdb/nt -out merged_hits.fa 

```


## Extract taxid of all unique BLAST hits

This loops through all out files in the output subfolders and merges the unique tax_ids into hits

The awk call splits the primer name to remove the chunk ID (ie primer_1 > primer 1 ), then prints just the primer and tax_ids, then call unique
```{bash}
echo $() >  hits.txt
for d in ./*_output/ ; do (cd "$d" && zcat *.out.gz | awk '{split($1,a,"_"); print a[1], $5}'| sort -u >> ../hits.txt ); done
cat hits.txt | sort -u > merged_hits.txt
pigz merged_hits.txt
rm hits.txt

```


## Process hits

```{r process blast taxonomy}
# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  mutate(seq = str_replace_all(seq, "I","N")) %>% 
  select(-score, -issues) %>%
  distinct()


hits <- vroom::vroom("primer_evaluation/merged_hits.txt.gz", col_names=c("primer", "tax_id"))

db <- taxreturn::get_ncbi_lineage()

off_target <- hits %>% 
  left_join(db, by="tax_id") %>%
  mutate(is_arthropod = case_when(
    phylum=="Arthropoda" ~ TRUE,
    !phylum=="Arthropoda" ~ FALSE
  ), is_insect = case_when(
    class=="Insecta" ~ TRUE,
    !class=="Insecta" ~ FALSE
  )) %>%
  group_by(primer)%>%
  summarise(freq_arthropod=(sum(is_arthropod, na.rm = TRUE)/n()),
            freq_insect=(sum(is_insect, na.rm = TRUE)/n()),
            n=n()
            ) %>%
  arrange(freq_arthropod) %>%
  mutate(name = primer) %>%
  left_join(primers)

#Relationshup with degeneracy
off_target %>%
  pivot_longer(starts_with("freq"),
               names_to = "type",
               values_to = "freq") %>%
  mutate(primer = as.factor(primer),
         primer = fct_reorder(primer, freq, .desc=FALSE)) %>%
  ggplot(aes(x=primer, y=freq, fill=degeneracy)) +
  geom_col() +
  facet_grid(~type) +
  base_theme +
  coord_flip()+
  theme(legend.position = "bottom") +
  scale_fill_gradient(low = "darkslateblue" , high ="firebrick", 
                      na.value = "grey",limits=c(0, 1000), oob = scales::squish)+
  labs(title = "Fill = Degeneracy",
       fill = "Primer Degeneracy")


# See if its capturing primer length:
off_target %>%
  pivot_longer(starts_with("freq"),
               names_to = "type",
               values_to = "freq") %>%
  mutate(primer = as.factor(primer),
         primer = fct_reorder(primer, freq, .desc=FALSE)) %>%
  ggplot(aes(x=primer, y=freq, fill=length)) +
  geom_col() +
  facet_grid(~type) +
  base_theme +
  coord_flip()+
  theme(legend.position = "bottom") +
  scale_fill_gradient(low = "darkslateblue" , high ="firebrick", 
                      na.value = "grey")+
  labs(title = "Fill = Primer Length",
       Fill = "Primer length")

# Why are some missing?
```

# PrimerTree

Note: Detailed information about installing and running PrimerTree can be found at https://github.com/jimhester/primerTree

Could use a slurm array to submit each seperately with all combinatons. 
Also worth having a function to count degeneracy for the constraints section
```{r off target}
primers <- read_csv("primer_evaluation/primer_candidates.csv")
library(primerTree)


forward <- "GGDRCWGGWTGAACWGTWTAYCCNCC"
rev <- "TATDGTRATDGCHCCNGC"

test <- search_primer_pair(
  forward,
  rev,
  api_key ="1c0a0c4afa28448650a1450662a22c68f208",
  num_permutations = 20
)
ranks = c("kingdom", "phylum", "class", 
    "order", "family", "genus", "species")

lineage <- test3[["taxonomy"]] %>%
  select(all_of(ranks))%>% 
  tidyr::unite(col = pathString, 
  !!ranks, sep = "/") %>%
  dplyr::mutate(pathString = paste0("Root/", pathString)) %>%
  data.tree::as.Node(.)

tree <- ape::read.tree(textConnection(data.tree::ToNewick(lineage, heightAttribute = NULL)))

library(ggtree)

ggtree(tree)


dir.create("PrimerTree")
#for (i in 1:nrow(dat.I)) {
  assign(paste("PT", dat.I$Name[i], sep = "."), search_primer_pair(name = dat.I$Name[i], dat.I$F.seq[i], dat.I$R.seq[i], num_permutations = 50, num_aligns = 1000))
  saveRDS(paste("PT", dat.I$Name[i], sep = "."),paste0("PrimerTree/PT.",dat.I$Name[i],".rds"))
}
#, clustal_options = c(exec='clustal-omega-1.2.2-win64/clustalo.exe')

# 3.2. - Inspect the sequence length distribution for each primer pair and remove any sequence records with a length deviating from the
#        majority of the sequences.

#Below code requires clustal files in R install directory

seq_lengths(`PT.AgPestF1-AgPestR1a`) # No obvious outliers
`PT.AgPestF1-AgPestR1a` <- filter_seqs(`PT.AgPestF1-AgPestR1a`, min_length = 200)

seq_lengths(`PT.AgPestF2-AgPestR2`) # No obvious outliers
`PT.AgPestF2-AgPestR2` <- filter_seqs(`PT.AgPestF2-AgPestR2`, min_length = 200)

seq_lengths(`PT.fwhF2-fwhR2n`) # No obvious outliers
`PT.fwhF2-fwhR2n` <- filter_seqs(`PT.fwhF2-fwhR2n`, min_length = 200)

seq_lengths(`PT.SauronS878-BR1`) # No obvious outliers
`PT.SauronS878-BR1` <- filter_seqs(`PT.SauronS878-BR1`, min_length = 200)

#Plot trees
t1 <- plot(`PT.AgPestF1-AgPestR1a`, ranks='class', main='PT.AgPestF1-AgPestR1a', rotate=45, size=1)
t2 <- plot(`PT.AgPestF2-AgPestR2`, ranks='class', main='PT.AgPestF2-AgPestR2', rotate=45, size=1)
t3 <- plot(`PT.fwhF2-fwhR2n`, ranks='class', main='PT.fwhF2-fwhR2n', rotate=45, size=1)
t4 <- plot(`PT.SauronS878-BR1`, ranks='class', main='PT.SauronS878-BR1', rotate=45, size=1)

Fig3 <- t1+t2+t3+t4
#on this plot we can see: 
#Off target amplifications
#Longer branch length = higher resolution


# 3.3. - Evaluate the taxonomic coverage and the specificity of the primers within the Actinopterygii class (i.e. Actinopteri class based
#        on the NCBI nomenclature). Also evaluate the taxonomic resolution of the primers at the genus level and correct for length of the
#        barcode to allow for comparisons between primers. Add additional columns to the input file for all calculated statistics.

dat.I$PT.Specificity <- rep("", nrow(dat.I))
dat.I$PT.PWDistance <- rep("", nrow(dat.I))
dat.I$PT.Length <- rep("", nrow(dat.I))
dat.I$PT.Resolution <- rep("", nrow(dat.I))
dat.I$PT.Order <- rep("", nrow(dat.I))
dat.I$PT.Family <- rep("", nrow(dat.I))
dat.I$PT.Genus <- rep("", nrow(dat.I))

for (i in 1:nrow(dat.I)) {
  tmp1 <- paste("PT", dat.I$Name[i], sep = ".")
  TAXID.All <- length(unique(as.data.frame(get(tmp1)$taxonomy)$taxId))
  TAXID.Act <- length(unique(subset(as.data.frame(get(tmp1)$taxonomy), subset = class == "Insecta")$taxId))
  dat.I$PT.Specificity[i] <- round((TAXID.Act / TAXID.All) * 100, digits = 2)
  dat.I$PT.PWDistance[i] <- as.numeric(calc_rank_dist_ave(get(tmp1), ranks = c("genus")))
  dat.I$PT.Length[i] <- as.integer(mean(get(tmp1)$BLAST_result$product_length))
  dat.I$PT.Resolution[i] <- as.numeric(dat.I$PT.PWDistance[i]) / as.numeric(dat.I$PT.Length[i])
  dat.I$PT.Order[i] <- length(unique(subset(as.data.frame(get(tmp1)$taxonomy), subset = class == "Insecta")$order))
  dat.I$PT.Family[i] <- length(unique(subset(as.data.frame(get(tmp1)$taxonomy), subset = class == "Insecta")$family))
  dat.I$PT.Genus[i] <- length(unique(subset(as.data.frame(get(tmp1)$taxonomy), subset = class == "Insecta")$genus))
}

#   B. Primer specificity (i.e. the percentage of unique Actinopterygii species out of the total number of unique species recovered)

gg.specificity <- ggplot(dat.I, aes(x = factor(Name, levels = Name), y = as.numeric(PT.Specificity))) +
  geom_bar(stat = "identity", fill = "gray50") +
  geom_hline(aes(yintercept = 90), linetype = "dashed", colour = "red3") +
  coord_cartesian(ylim = c(30, 100)) +
  ggtitle("B") +
  ylab("% of unique Actinopterygii species") +
  theme_classic() +
  theme(
    plot.title = element_text(size = 8, colour = "black", face = "bold"),
    axis.title.x = element_blank(),
    axis.title.y = element_text(size = 7, colour = "black"),
    axis.text.x = element_text(size = 7, colour = "black", angle = 90, hjust = 1, vjust = 0.5),
    axis.text.y = element_text(size = 7, colour = "black"),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_line(size = 0.25, colour = "black"),
    axis.ticks.length = unit(0.1, "cm")
  )
gg.specificity

#   C. Taxonomic coverage (i.e. no. of Actinopterygii orders for which sequences were obtained)

gg.coverage <- ggplot(dat.I, aes(x = factor(Name, levels = Name), y = as.numeric(PT.Order))) +
  geom_bar(stat = "identity", fill = "gray50") +
  geom_hline(aes(yintercept = 30), linetype = "dashed", colour = "red3") +
  coord_cartesian(ylim = c(0, 40)) +
  ggtitle("C") +
  ylab("No. of Actinopterygii orders") +
  theme_classic() +
  theme(
    plot.title = element_text(size = 8, colour = "black", face = "bold"),
    axis.title.x = element_blank(),
    axis.title.y = element_text(size = 7, colour = "black"),
    axis.text.x = element_text(size = 7, colour = "black", angle = 90, hjust = 1, vjust = 0.5),
    axis.text.y = element_text(size = 7, colour = "black"),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_line(size = 0.25, colour = "black"),
    axis.ticks.length = unit(0.1, "cm")
  )
gg.coverage

```


```{r sessioninfo}
sessionInfo(package = NULL)
```
